---
title: "Practical Implementations of Principal Component Analysis"
author: "Shivani"
date: "2024-08-07"
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


[View Slideshow](slides.html)

## Introduction

Principal Component Analysis (PCA) is a pivotal multivariate data analysis technique employed to simplify complex datasets by reducing their dimensionality while retaining critical information. 
PCA has become an essential tool in statistical analysis and data science for its ability to identify patterns and relationships within high-dimensional data [@Principalcomponentanalysiswithlinearalgebra].

PCA leverages the mathematical framework of linear algebra, specifically the eigen-decomposition of covariance or correlation matrices, to transform a set of possibly correlated variables into a smaller number of uncorrelated variables known as principal components. 

This transformation is achieved by identifying the eigenvectors and eigenvalues of the matrix, where eigenvectors define the directions of maximum variance and eigenvalues indicate the magnitude of variance along these directions. [@MultivariateStatisticalDataAnalysis].


## Related work

The development of Principal Component Analysis (PCA) can be traced back to the foundational work of Pearson (1901), who introduced the concept of reducing dimensionality in multivariate datasets by identifying linear projections that maximize variance. Pearson's pioneering work set the stage for further advancements, who formalized the method of principal components through an algebraic approach. Hotelling‚Äôs work introduced the concept of eigenvectors and eigenvalues in the context of data variance and highlighted the utility of PCA in extracting significant features from complex datasets [@bro2014principal_biplot_theoretical_preprocessing].



## Methods

While different methods can be used to determine the principal components, Singular Value Decomposition (SVD) is the most common due to its computational efficiency and numerical stability. SVD decomposes the data into three simpler matrices, making it possible to handle large datasets effectively. This decomposition allows for a more straightforward calculation of the principal components without explicitly computing the covariance matrix, which can be computationally expensive and numerically unstable. The ability to handle sparse and dense matrices further enhances its versatility and efficiency in diverse applications. These advantages make SVD a preferred method in popular programming packages, ensuring that PCA is performed quickly and accurately [@johnson2023applied].

### Overview of Algorithm with SVD:  

1. **Standardize the data**  
   Ensure each variable contributes equally by having a mean of zero and variance of one. This requires that the variables are continuous.
   $$
   X_\text{standardized} = \frac{X - \mu}{\sigma}
   $$
   $\mu$: mean, $\sigma$: standard deviation of each variable

2. **Perform Singular Value Decomposition (SVD)**  
  Decompose the standardized data matrix into three matrices $U$, $\Sigma$, and $V^T$.
  $$
  X_\text{standardized} = U \Sigma V^T
  $$
  $ùëà$ and $V^T$ are orthogonal matrices, so therefore $U$ and $V$ are also orthogonal. The diagonal matrix of singular values is represented by $\sigma$ with values $\sigma_i$ that are naturally sorted in descending order. Each column of $ùëâ$ represents a principal component (PC) which are orthogonal to each other in the transformed feature space. The number of PC's initially generated are equivalent to the number of variables initially provided in the dataset.

3. **Selection of principal components**  
  The explained variance of each PC is represented by:
  $$
  \text{variance explained} = \frac{\sigma_i^2}{\sum \sigma_i^2}
  $$
  For each singular value $\sigma_i$ in the matrix of singular values $\Sigma$, compute $\sigma_i^2$ and calculate the individual variance. The cumulative explained variance target is specified (typically 95%) as a criteria for PC selection. Determine the number of components needs to reach the desired cumulative explained variance.

4. **Transform the data**  
  Project the data onto the selected principal components. This yields the new subspace with reduced dimensions.
  $$
  X_\text{transformed} = X_\text{standardized} V^T_\text{selected}
  $$
  where $V_\text{selected}$ contains the first number of components columns of $V$. Once the data is transformed onto the selected PC's, it is effectively reduced in dimensionality while retaining most of its original variability.



## Analysis and Results

### Application - PCA on USDA National Nutrient data

### Dataset

Each record is for 100 grams.

The nutrient columns end with the units, so:

Nutrient_g is in grams Nutrient_mg is in milligrams Nutrient_mcg is in micrograms Nutrient_USRDA is in percentage of US Recommended Daily Allows (e.g. 0.50 is 50%)



```{r}
#Install Libraries
#install.packages("devtools")
#library(devtools)
#install_version("MASS", version = "7.3-60")  # Replace with a compatible version

#install.packages("MASS")
```

### Check for highly correlated features

Removing _USRD records as the data is redundant and also selecting highly corrrelated data.

```{r}
library(MASS)
library(factoextra)
library(ggplot2)
library(readr)

library(dplyr)
library(caret)
library(tibble)

url <- "https://query.data.world/s/ll77ildgnhhove7mlker3g2jw7z5qr?dws=00000"
data <- read.csv(url, header=TRUE, stringsAsFactors=FALSE)
str(data)
summary(data)

```

```{r}
data_omit_usrda <- data %>% dplyr::select(-contains('_USRDA'))

str(data_omit_usrda)

data_numeric <- data_omit_usrda %>% dplyr::select(where(is.numeric))

df <- data_numeric %>% column_to_rownames('ID')
df_desc <- df[, 1:6]
data_numeric <- df[, -c(1:6)]
#print(data_numeric, 15)

```


### Scale Data

-   Scale data to center and/or scale the columns of a numeric dataframe.
-   Standardize variables to have a mean of zero and a standard deviation of one

```{r}
data_scaled <- scale(data_numeric)
names(data_numeric)

#print(data_scaled)
cat("mean:", round(mean(data_scaled), 2), "\n")
cat("standard dev:", round(sd(data_scaled), 2), "\n")

```

### Corelation check

- Check if features in the data are correlated


```{r}
cor(data_scaled)
mean(cor(data_scaled))

```


### Apply Prinicipal Component Analysis

- Running PCA generated multiple Principal components

- Number of Principal components will be equal to number of variables in the data.    

- First few components will be important.

```{r}
pca <- prcomp(data_scaled, scale = TRUE)

pca$rotation


```

```{r}
#qplot(1:length(pca$sdev^2), pca$sdev^2 / sum(pca$sdev^2), geom="line")

```

```{r}
explained_variance <- pca$sdev^2 / sum(pca$sdev^2)
cat(explained_variance, "\n")
cat(sum(explained_variance[1:6]), "\n")
```

**Scree Plot**

-   The scree plot shows the proportion of variance explained by each principal component.
-   This plot helps in deciding how many components to retain for further analysis.
-   First seven components explain most of the variance, (\> 70%)

```{r}
# Visualize the PCA results
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50))

```


### BiPlot

```{r}

col_unique <- as.character(data$FoodGroup)

color_values <- setNames(rainbow(length(unique(col_unique))), unique(col_unique))


fviz_pca_biplot(pca, geom.ind = "point", pointsize = 2.5 , col.var = "black" ,
                col.ind =col_unique ,  repel = TRUE)+
  scale_color_manual(name = "Food Groups", labels = unique(col_unique), values = color_values)
```


### Visualization

Now let's look at which food groups are highest in each component

```{r}
pca_results <- as.data.frame(pca$x)
pca_results$FoodGroup <- data$FoodGroup

plot_top_foodgroups <- function(pca_results, component, n = 10) {
  top_foodgroups <- pca_results %>%
    arrange(desc(!!sym(component))) %>%
    slice(1:n) %>%
    count(FoodGroup, sort = TRUE)
  
  ggplot(top_foodgroups, aes(x = reorder(FoodGroup, n), y = n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = paste("Top Food Groups for", component),
         x = "Food Group", y = "Count")
}

```

### Creating dataset with pricipal components

-   Creating dataset with 7 PCs

```{r}
pca_df <- as.data.frame(pca$x[, 1:7])
pca_df <- rownames_to_column(pca_df, var = "ID")
names(pca_df)
pca_new <- prcomp(pca_df[, 2:8])
#cor(pca_df[, 2:8])
#mean(cor(pca_df[, 2:8]))
fviz_pca_biplot(pca_new, geom.ind = "point", pointsize = 2.5 , col.var = "black" ,
                col.ind =col_unique ,  repel = TRUE)+
  scale_color_manual(name = "Food Groups", labels = unique(col_unique), values = color_values)

```

### Analysis of PCA Components

PC1

```{r}
  component <- paste0("PC", 1)
  print(plot_top_foodgroups(pca_results, component))

```

---

PC2

```{r}
  component <- paste0("PC", 2)
  print(plot_top_foodgroups(pca_results, component))

```

PC3

```{r}
  component <- paste0("PC", 3)
  print(plot_top_foodgroups(pca_results, component))

```

PC4

```{r}
  component <- paste0("PC", 4)
  print(plot_top_foodgroups(pca_results, component))

```

PC5

```{r}
  component <- paste0("PC", 5)
  print(plot_top_foodgroups(pca_results, component))

```



Foods that are high in: VitC and Manganese

Low in: Niacin_mg Riboflavin_mg

```{r}
vects <- pca$rotation[, 1:5]
component_one <- vects[, 1]
print(sort(component_one, decreasing = TRUE))

```

Foods that are high in: VitA and VitB12

Low in: Folate and Thiamin

```{r}

component_two <- vects[, 2]
print(sort(component_two, decreasing = TRUE))

```

## Summary of Test results

Dimensionality Reduction:

-   By reducing the dataset to a few principal components, this helps in simplifying the dataset while retaining most of the important information.

Top Contributing Food Groups:

-   The bar plots for each of the first five principal components indicate which food groups are most influential in each component. 

-   This helps in understanding the underlying structure of the data and identifying patterns or groups of foods that are similar in their nutritional profiles.


## Conclusion


Principal Component Analysis (PCA) is an invaluable tool in the realm of data analysis. 

Through its mathematical foundation in linear algebra, PCA transforms high-dimensional data into a set of orthogonal principal components, each representing a direction of maximum variance within the dataset. [@Thecurseofdimensionality]

This transformation not only simplifies data complexity but also facilitates improved interpretation and visualization, making PCA a crucial technique in different fields.

Despite its strengths, PCA is not without limitations. The linear nature of PCA may not capture complex, non-linear relationships within data, and the interpretation of principal components can sometimes be challenging. 

PCA remains a fundamental technique in multivariate analysis, providing essential insights into data structure and aiding in the development of robust analytical models. As the volume and complexity of data continue to grow, PCA's role in facilitating efficient data analysis and decision-making will be more critical . 

